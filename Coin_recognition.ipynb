{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coin_recognition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3s_4MLIQAMT"
      },
      "source": [
        "Połączenie z dyskiem "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh5Yz-XfzudL"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "import json\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM1tf5vhUyDn"
      },
      "source": [
        "Przygotowywanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FeNAeIvxHUr"
      },
      "source": [
        "def getListOfFiles(dirName):\n",
        "    # create a list of file and sub directories \n",
        "    # names in the given directory \n",
        "    listOfFile = os.listdir(dirName)\n",
        "    allFiles = list()\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "        # Create full path\n",
        "        fullPath = os.path.join(dirName, entry)\n",
        "        # If entry is a directory then get the list of files in this directory \n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "                \n",
        "    return allFiles\n",
        "\n",
        "dirName='/content/drive/MyDrive/UM_baza_danych/50gr'\n",
        "listOfFiles = getListOfFiles(dirName)\n",
        "counter = 0\n",
        "width_resized = 128\n",
        "height_resized = 128\n",
        "dim = (width_resized, height_resized)\n",
        "\n",
        "for file in listOfFiles:\n",
        "  img = cv2.imread(file)\n",
        "  counter += 1\n",
        "  height, width, channels = img.shape\n",
        "  img = img[int(height/3.5):int(height-height/3.5), int(width/3.5):int(width-width/3.5)]\n",
        "  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "  cv2.imwrite('/content/drive/MyDrive/UM_baza_danych/resized/50gr/' + os.path.basename(file), resized)\n",
        "  print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8Y9YVoOc_c"
      },
      "source": [
        "Pobierz set zdjęć"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXFCMk5jGdJo"
      },
      "source": [
        "! cp -r /content/drive/MyDrive/UM_baza_danych/resized /home"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_lJJ0UfOV_r"
      },
      "source": [
        "Pobierz set zdjęć testowych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOuPu_A5IkKa"
      },
      "source": [
        "! cp -r /content/drive/MyDrive/UM_baza_danych/RandomCoins /home"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gopufy1VQDlr"
      },
      "source": [
        "Obróbka zdjęć\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xvdJe2AUIMP"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "epochs = 30\n",
        "seed = 123\n",
        "\n",
        "def train_data():\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        seed=seed,\n",
        "        shuffle=False,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        "    return train_ds\n",
        "\n",
        "\n",
        "def val_data():\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        seed=seed,\n",
        "        shuffle=False,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        "    return val_ds\n",
        "\n",
        "def plot_val_train(epochs, history):\n",
        "\n",
        "    if (type(history) is dict):\n",
        "\n",
        "      acc = history['accuracy']\n",
        "      val_acc = history['val_accuracy']\n",
        "\n",
        "      loss = history['loss']\n",
        "      val_loss = history['val_loss']\n",
        "   \n",
        "    else:\n",
        "\n",
        "      acc = history.history['accuracy']\n",
        "      val_acc = history.history['val_accuracy']\n",
        "\n",
        "      loss = history.history['loss']\n",
        "      val_loss = history.history['val_loss']\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.axhline(y = 0.994, color = 'r', linestyle = '-', label = \"99,4%\")\n",
        "    plt.legend(loc='right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "def save_val_train(epochs, history, model, date, number_of_filters):\n",
        "\n",
        "    architecture = ''\n",
        "    for layer in model.layers:\n",
        "      architecture += layer.name + \"__\"\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.savefig('/content/drive/MyDrive/UM_baza_danych/{}/'.format(date) + str(architecture) + str(number_of_filters) + '.png')\n",
        "\n",
        "def plot_starting_image():\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in train_ds.take(1):\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            plt.title(class_names[labels[i]])\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "images_classes_path = '/home/resized'\n",
        "\n",
        "data_dir = pathlib.Path(images_classes_path)\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)\n",
        "\n",
        "NAME = \"COINS-CNN\"\n",
        "train_ds = train_data()\n",
        "val_ds = val_data()\n",
        "class_names = train_ds.class_names\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "test_dataset = val_ds.take(val_batches // 5)\n",
        "validation_dataset = val_ds.skip(val_batches // 5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaWvSCv7QG3r"
      },
      "source": [
        "Trenowanie modelu "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F22YN6E2bXy8"
      },
      "source": [
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
        "\n",
        "num_classes = 4\n",
        "epochs = 30\n",
        "number_of_filters = 16\n",
        "model = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1. / 255),\n",
        "    layers.Conv2D(number_of_filters, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(number_of_filters*2, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(number_of_filters*4, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(number_of_filters*4, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"/home/logs/{}\".format(NAME))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(\n",
        "      optimizer=opt,\n",
        "      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=validation_dataset,\n",
        "      epochs=epochs\n",
        "  )\n",
        "\n",
        "t = datetime.datetime.now()\n",
        "export_path = \"/content/drive/MyDrive/UM_baza_danych/{}\".format(t)\n",
        "history_dict = history.history\n",
        "\n",
        "model.save(export_path + '/weights')\n",
        "json.dump(history_dict, open(export_path + '/history', 'w'))\n",
        "save_val_train(epochs, history, model, t, number_of_filters)\n",
        "\n",
        "model.summary()\n",
        "with open(export_path + '/model.txt', 'w') as f:\n",
        "  with redirect_stdout(f):\n",
        "    model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyAC7Nb1OmDE"
      },
      "source": [
        "Załaduj model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glPPl62JgKSX"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/UM_baza_danych/2021-05-13 18:26:23.174296_4conv_128/weights\")\n",
        "history = json.load(open(\"/content/drive/MyDrive/UM_baza_danych/2021-05-13 18:26:23.174296_4conv_128/history\", 'r'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U5o5F6nwRAQ"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tr39GE5Oo4V"
      },
      "source": [
        "Pokaż przykładowe obrazy przed treningiem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90i7c_EZGGfZ"
      },
      "source": [
        "plot_starting_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvbzCQI5OvLf"
      },
      "source": [
        "Pokaż wykresy rezultatów"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-7g0dQpFym5"
      },
      "source": [
        "plot_val_train(epochs,history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-PmdsVp4U8t"
      },
      "source": [
        "Sprawdź model i źle zaklasyfikowane zdjęcia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxoQkKI1nU1M"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_dataset)\n",
        "print('Test accuracy :', accuracy)\n",
        "\n",
        "a = 0\n",
        "img_count = 0\n",
        "errors = 0\n",
        "max_score_correct = 0\n",
        "min_score_correct = 1\n",
        "max_score_false = 0\n",
        "min_score_false = 1\n",
        "plt.figure(figsize=(40, 40))\n",
        "\n",
        "for i in range(tf.data.experimental.cardinality(validation_dataset)):\n",
        "  image_batch, label_batch = validation_dataset.as_numpy_iterator().next()\n",
        "  predictions = model.predict(image_batch)\n",
        "  for j in range(batch_size):\n",
        "    img_count += 1\n",
        "    score = tf.nn.softmax(predictions[j])\n",
        "\n",
        "    if(label_batch[j] == np.argmax(score)):\n",
        "      if (np.max(score) > max_score_correct):\n",
        "        max_score_correct = np.max(score)\n",
        "      if (np.max(score) < min_score_correct):\n",
        "        min_score_correct = np.max(score)\n",
        "\n",
        "    if(label_batch[j] != np.argmax(score)):\n",
        "      errors += 1\n",
        "      if (np.max(score) > max_score_false):\n",
        "        max_score_false = np.max(score)\n",
        "      if (np.max(score) < min_score_false):\n",
        "        min_score_false = np.max(score)\n",
        "      \n",
        "\n",
        "      ax = plt.subplot(10, 10, a + 1)\n",
        "      plt.imshow(image_batch[j].astype(\"uint8\"))\n",
        "      plt.title(class_names[np.argmax(score)])\n",
        "      a += 1\n",
        "\n",
        "print('Test accuracy : {}%'.format(errors/img_count*100))\n",
        "print(\"Maksymalna pewność modelu dla poprawnej klasyfikacji: \", max_score_correct)\n",
        "print(\"Minimalna pewność modelu dla poprawnej klasyfikacji: \", min_score_correct)\n",
        "print(\"Maksymalna pewność modelu dla błędnej klasyfikacji: \", max_score_false)\n",
        "print(\"Minimalna pewność modelu dla błędnej klasyfikacji: \", min_score_false)\n",
        "print(img_count)\n",
        "print(errors)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgHvC6w-PmSr"
      },
      "source": [
        "Testowanie wytrenowanego modelu na losowych zdjęciach spoza bazy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsUBBOgIqdD"
      },
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "j = 0;\n",
        "for i in os.listdir('/home/RandomCoins'):\n",
        "    ax = plt.subplot(4, 4, j + 1)\n",
        "    img = tf.keras.preprocessing.image.load_img('/home/RandomCoins/'+i, target_size=(128, 128))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "    img = tf.keras.preprocessing.image.load_img('/home/RandomCoins/'+i, target_size=(512, 512))\n",
        "    plt.imshow(img)\n",
        "    plt.title(class_names[np.argmax(score)])\n",
        "    j+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8z7YX0FNKtx"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import models\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # cv2.imshow does not work on Google Colab notebooks, which is why we are using cv2_imshow instead\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def do_heat_map(img_user, flag):\n",
        "\n",
        "  if (flag == \"src\"):\n",
        "    image_src = img_user\n",
        "    # Load pre-trained Keras model and the image to classify\n",
        "    image = tf.keras.preprocessing.image.load_img(image_src, target_size=(128, 128))\n",
        "  else:\n",
        "    image = img_user\n",
        "  img_tensor = preprocessing.image.img_to_array(image)\n",
        "  img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "  img_tensor = preprocess_input(img_tensor)\n",
        "\n",
        "  conv_layer = model.get_layer(\"conv2d_7\")\n",
        "  heatmap_model = models.Model([model.inputs], [conv_layer.output, model.output])\n",
        "\n",
        "  # Get gradient of the winner class w.r.t. the output of the (last) conv. layer\n",
        "  with tf.GradientTape() as gtape:\n",
        "      conv_output, predictions = heatmap_model(img_tensor)\n",
        "      loss = predictions[:, np.argmax(predictions[0])]\n",
        "      grads = gtape.gradient(loss, conv_output)\n",
        "      pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "  heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
        "  heatmap = np.maximum(heatmap, 0)\n",
        "  max_heat = np.max(heatmap)\n",
        "  if max_heat == 0:\n",
        "      max_heat = 1e-10\n",
        "  heatmap /= max_heat\n",
        "  hm=np.squeeze(heatmap)\n",
        "  plt.imshow(hm)\n",
        "  if (flag == \"src\"):\n",
        "    img = cv2.imread(image_src)\n",
        "  else:\n",
        "    img = img_user\n",
        "  INTENSITY = 0.5\n",
        "\n",
        "  heatmap = cv2.resize(hm, (img.shape[1], img.shape[0]))\n",
        "\n",
        "  heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "\n",
        "  img = heatmap * INTENSITY + img\n",
        "  img = np.float32(img)\n",
        "  # cv2_imshow(img)\n",
        "  return img\n",
        "\n",
        "def show_heat_random():\n",
        "  # plt.figure(figsize=(16,16))\n",
        "  # j = 0;\n",
        "  for i in os.listdir('/home/RandomCoins/'):\n",
        "      # plt.subplot(4, 8, 2*j + 1)\n",
        "      image1 = cv2.imread('/home/RandomCoins/'+i)\n",
        "      height, width, channels = image1.shape\n",
        "      image2 = do_heat_map('/home/RandomCoins/'+i, \"src\")\n",
        "\n",
        "      cv2_imshow(image1)\n",
        "      cv2_imshow(image2)\n",
        "\n",
        "\n",
        "\n",
        "def show_heat_test():\n",
        "  image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
        "  for i in range(9):\n",
        "    image1 = (image_batch[i].astype(\"uint8\"))\n",
        "    image1 = cv2.resize(image1, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "    cv2_imshow(cv2.cvtColor(image1, cv2.COLOR_RGB2BGR))\n",
        "    image2 = do_heat_map(image_batch[i].astype(\"uint8\"), \"nosrc\")\n",
        "    image2 = cv2.resize(image2, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "    cv2_imshow(image2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPtjYnNrITM2"
      },
      "source": [
        "show_heat_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW1xnaOUIYbO"
      },
      "source": [
        "show_heat_random()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}